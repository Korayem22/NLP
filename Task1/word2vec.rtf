{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww28300\viewh16080\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Efficient Estimation of Word Representations in Vector Space (Word2Vec)\
\
Objective:\
The objective of the Word2Vec paper is to propose efficient methods to learn high-quality word embeddings from large datasets. These embeddings capture semantic meanings of words such that similar words are mapped to nearby points in the vector space.\
\
How to Achieve It:\
1. Continuous Bag-of-Words (CBOW): Predicts a target word based on its context (surrounding words).\
2. Skip-Gram: Predicts context words given a target word.\
\
The models are trained using neural networks without hidden layers, which significantly improves training efficiency. Techniques like hierarchical softmax and negative sampling are used to reduce computational complexity.\
\
Performance is measured using:\
1. Word similarity tasks: Comparing cosine similarity of word vectors against human-judged similarity scores.\
2. Word analogy tasks: Testing vector arithmetic (e.g., "king" - "man" + "woman" \uc0\u8776  "queen").\
3. Downstream NLP tasks: Using embeddings in applications like named entity recognition or sentiment analysis to observe performance improvements.\
Word2Vec provides a scalable and effective method to learn word embeddings that capture rich semantic relationships, and it significantly outperforms previous models in both accuracy and computational efficiency.\
\
\
}