{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sa240\partightenfactor0

\f0\fs24 \cf0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Key points I noticed in the paper\
1- The paper compares the performance of vanilla RNNs, LSTMs, and GRUs on sequence modeling tasks.\
2- The experiments are conducted on three public polyphonic music datasets and two proprietary speech datasets from Ubisoft.\
3- It explains the basic architectures of RNN, LSTM, and GRU, detailing their structural differences.\
4- The main distinction between LSTM and GRU is that GRU does not use an output gate controlled by a sigmoid function.\
5- GRUs have a simpler structure than LSTMs, resulting in fewer parameters.\
6- When using approximately the same number of parameters across models, GRUs perform comparably to LSTMs while requiring less computational power to converge.\
}